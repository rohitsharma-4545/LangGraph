{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "040b03f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jeste\\OneDrive\\Desktop\\JES\\LangGraph\\myenv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:26: UserWarning: Core Pydantic V1 functionality isn't compatible with Python 3.14 or greater.\n",
      "  from pydantic.v1.fields import FieldInfo as FieldInfoV1\n",
      "c:\\Users\\jeste\\OneDrive\\Desktop\\JES\\LangGraph\\myenv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import operator\n",
    "import os\n",
    "import re\n",
    "from datetime import date, timedelta\n",
    "from pathlib import Path\n",
    "from typing import TypedDict, List, Optional, Literal, Annotated\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.types import Send\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "from langchain_tavily import TavilySearch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ecd8333",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 1) Schemas\n",
    "# -----------------------------\n",
    "class Task(BaseModel):\n",
    "    id: int\n",
    "    title: str\n",
    "\n",
    "    goal: str = Field(\n",
    "        ...,\n",
    "        description=\"One sentence describing what the reader should be able to do/understand after this section.\",\n",
    "    )\n",
    "    bullets: List[str] = Field(\n",
    "        ...,\n",
    "        min_length=3,\n",
    "        max_length=6,\n",
    "        description=\"3–6 concrete, non-overlapping subpoints to cover in this section.\",\n",
    "    )\n",
    "    target_words: int = Field(..., description=\"Target word count for this section (120–550).\")\n",
    "\n",
    "    tags: List[str] = Field(default_factory=list)\n",
    "    requires_research: bool = False\n",
    "    requires_citations: bool = False\n",
    "    requires_code: bool = False\n",
    "\n",
    "\n",
    "class Plan(BaseModel):\n",
    "    blog_title: str\n",
    "    audience: str\n",
    "    tone: str\n",
    "    blog_kind: Literal[\"explainer\", \"tutorial\", \"news_roundup\", \"comparison\", \"system_design\"] = \"explainer\"\n",
    "    constraints: List[str] = Field(default_factory=list)\n",
    "    tasks: List[Task]\n",
    "\n",
    "\n",
    "class EvidenceItem(BaseModel):\n",
    "    title: str\n",
    "    url: str\n",
    "    published_at: Optional[str] = None  # keep if Tavily provides; DO NOT rely on it\n",
    "    snippet: Optional[str] = None\n",
    "    source: Optional[str] = None\n",
    "\n",
    "\n",
    "class RouterDecision(BaseModel):\n",
    "    needs_research: bool\n",
    "    mode: Literal[\"closed_book\", \"hybrid\", \"open_book\"]\n",
    "    queries: List[str] = Field(default_factory=list)\n",
    "\n",
    "\n",
    "class EvidencePack(BaseModel):\n",
    "    evidence: List[EvidenceItem] = Field(default_factory=list)\n",
    "\n",
    "\n",
    "class ImageSpec(BaseModel):\n",
    "    placeholder: str = Field(..., description=\"e.g. [[IMAGE_1]]\")\n",
    "    filename: str = Field(..., description=\"Save under images/, e.g. qkv_flow.png\")\n",
    "    alt: str\n",
    "    caption: str\n",
    "    prompt: str = Field(..., description=\"Prompt to send to the image model.\")\n",
    "    size: Literal[\"1024x1024\", \"1024x1536\", \"1536x1024\"] = \"1024x1024\"\n",
    "    quality: Literal[\"low\", \"medium\", \"high\"] = \"medium\"\n",
    "\n",
    "\n",
    "class GlobalImagePlan(BaseModel):\n",
    "    md_with_placeholders: str\n",
    "    images: List[ImageSpec] = Field(default_factory=list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c5a0c988",
   "metadata": {},
   "outputs": [],
   "source": [
    "class State(TypedDict):\n",
    "    topic: str\n",
    "\n",
    "    # routing / research\n",
    "    mode: str\n",
    "    needs_research: bool\n",
    "    queries: List[str]\n",
    "    evidence: List[EvidenceItem]\n",
    "    plan: Optional[Plan]\n",
    "\n",
    "    # workers\n",
    "    sections: Annotated[List[tuple[int, str]], operator.add]  # (task_id, section_md)\n",
    "\n",
    "    # reducer/image\n",
    "    merged_md: str\n",
    "    md_with_placeholders: str\n",
    "    image_specs: List[dict]\n",
    "\n",
    "    final: str\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 2) LLM\n",
    "# -----------------------------\n",
    "llm = ChatOpenAI(\n",
    "    base_url=\"https://api.sambanova.ai/v1\",\n",
    "    api_key=os.getenv(\"DEEPSEEK_API_KEY\"),\n",
    "    model=\"Meta-Llama-3.3-70B-Instruct\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b085e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 3) Router (decide upfront)\n",
    "# -----------------------------\n",
    "ROUTER_SYSTEM = \"\"\"You are a routing module for a technical blog planner.\n",
    "\n",
    "Decide whether web research is needed BEFORE planning.\n",
    "\n",
    "Modes:\n",
    "- closed_book (needs_research=false):\n",
    "  Evergreen topics where correctness does not depend on recent facts (concepts, fundamentals).\n",
    "- hybrid (needs_research=true):\n",
    "  Mostly evergreen but needs up-to-date examples/tools/models to be useful.\n",
    "- open_book (needs_research=true):\n",
    "  Mostly volatile: weekly roundups, \"this week\", \"latest\", rankings, pricing, policy/regulation.\n",
    "\n",
    "If needs_research=true:\n",
    "- Output 3–6 high-signal queries.\n",
    "- Queries should be scoped and specific (avoid generic queries like just \"AI\" or \"LLM\").\n",
    "- If user asked for \"last week/this week/latest\", reflect that constraint IN THE QUERIES.\n",
    "\"\"\"\n",
    "\n",
    "def router_node(state: State) -> dict:\n",
    "    \n",
    "    topic = state[\"topic\"]\n",
    "    decider = llm.with_structured_output(RouterDecision)\n",
    "    decision = decider.invoke(\n",
    "        [\n",
    "            SystemMessage(content=ROUTER_SYSTEM),\n",
    "            HumanMessage(content=f\"Topic: {topic}\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"needs_research\": decision.needs_research,\n",
    "        \"mode\": decision.mode,\n",
    "        \"queries\": decision.queries,\n",
    "    }\n",
    "\n",
    "def route_next(state: State) -> str:\n",
    "    return \"research\" if state[\"needs_research\"] else \"orchestrator\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "20f7d8ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 4) Research (Tavily) \n",
    "# -----------------------------\n",
    "def _tavily_search(query: str, max_results: int = 5) -> List[dict]:\n",
    "    \n",
    "    tool = TavilySearch(max_results=max_results, tavily_api_key=os.getenv(\"TAVILY_API_KEY\"))\n",
    "    response = tool.invoke({\"query\": query})\n",
    "    results = response.get(\"results\", [])\n",
    "\n",
    "    normalized: List[dict] = []\n",
    "    for r in results or []:\n",
    "        normalized.append(\n",
    "            {\n",
    "                \"title\": r.get(\"title\") or \"\",\n",
    "                \"url\": r.get(\"url\") or \"\",\n",
    "                \"snippet\": r.get(\"content\") or r.get(\"snippet\") or \"\",\n",
    "                \"published_at\": r.get(\"published_date\") or r.get(\"published_at\"),\n",
    "                \"source\": r.get(\"source\"),\n",
    "            }\n",
    "        )\n",
    "    return normalized\n",
    "\n",
    "\n",
    "RESEARCH_SYSTEM = \"\"\"You are a research synthesizer for technical writing.\n",
    "\n",
    "Given raw web search results, produce a deduplicated list of EvidenceItem objects.\n",
    "Striclty Return ONLY valid JSON, do NOT include explanations, markdown, or extra text.\n",
    "\n",
    "Rules:\n",
    "- Only include items with a non-empty url.\n",
    "- Prefer relevant + authoritative sources (company blogs, docs, reputable outlets).\n",
    "- If a published date is explicitly present in the result payload, keep it as YYYY-MM-DD.\n",
    "  If missing or unclear, set published_at=null. Do NOT guess.\n",
    "- Keep snippets short.\n",
    "- Deduplicate by URL.\n",
    "\"\"\"\n",
    "\n",
    "def research_node(state: State) -> dict:\n",
    "\n",
    "    # take the first 10 queries from state\n",
    "    queries = (state.get(\"queries\", []) or [])\n",
    "    max_results = 6\n",
    "\n",
    "    raw_results: List[dict] = []\n",
    "\n",
    "    for q in queries:\n",
    "        raw_results.extend(_tavily_search(q, max_results=max_results))\n",
    "\n",
    "    if not raw_results:\n",
    "        return {\"evidence\": []}\n",
    "\n",
    "    extractor = llm.with_structured_output(EvidencePack)\n",
    "    pack = extractor.invoke(\n",
    "        [\n",
    "            SystemMessage(content=RESEARCH_SYSTEM),\n",
    "            HumanMessage(content=f\"Raw results:\\n{raw_results}\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Deduplicate by URL\n",
    "    dedup = {}\n",
    "    for e in pack.evidence:\n",
    "        if e.url:\n",
    "            dedup[e.url] = e\n",
    "\n",
    "    return {\"evidence\": list(dedup.values())}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e55d0a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 5) Orchestrator (Plan)\n",
    "# -----------------------------\n",
    "ORCH_SYSTEM = \"\"\"You are a senior technical writer and developer advocate.\n",
    "Your job is to produce a highly actionable outline for a technical blog post.\n",
    "\n",
    "Hard requirements:\n",
    "- Create 4-6 sections (tasks) suitable for the topic and audience.\n",
    "- Each task must include:\n",
    "  1) goal (1 sentence)\n",
    "  2) 3–5 bullets that are concrete, specific, and non-overlapping\n",
    "  3) target word count (120–450)\n",
    "\n",
    "Quality bar:\n",
    "- Assume the reader is a developer; use correct terminology.\n",
    "- Bullets must be actionable: build/compare/measure/verify/debug.\n",
    "- Ensure the overall plan includes at least 2 of these somewhere:\n",
    "  * minimal code sketch / MWE (set requires_code=True for that section)\n",
    "  * edge cases / failure modes\n",
    "  * performance/cost considerations\n",
    "  * security/privacy considerations (if relevant)\n",
    "  * debugging/observability tips\n",
    "\n",
    "Grounding rules:\n",
    "- Mode closed_book: keep it evergreen; do not depend on evidence.\n",
    "- Mode hybrid:\n",
    "  - Use evidence for up-to-date examples (models/tools/releases) in bullets.\n",
    "  - Mark sections using fresh info as requires_research=True and requires_citations=True.\n",
    "- Mode open_book:\n",
    "  - Set blog_kind = \"news_roundup\".\n",
    "  - Every section is about summarizing events + implications.\n",
    "  - DO NOT include tutorial/how-to sections unless user explicitly asked for that.\n",
    "  - If evidence is empty or insufficient, create a plan that transparently says \"insufficient sources\"\n",
    "    and includes only what can be supported.\n",
    "\n",
    "Output must strictly match the Plan schema.\n",
    "\"\"\"\n",
    "\n",
    "def orchestrator_node(state: State) -> dict:\n",
    "    planner = llm.with_structured_output(Plan)\n",
    "\n",
    "    evidence = state.get(\"evidence\", [])\n",
    "    mode = state.get(\"mode\", \"closed_book\")\n",
    "\n",
    "    plan = planner.invoke(\n",
    "        [\n",
    "            SystemMessage(content=ORCH_SYSTEM),\n",
    "            HumanMessage(\n",
    "                content=(\n",
    "                    f\"Topic: {state['topic']}\\n\"\n",
    "                    f\"Mode: {mode}\\n\\n\"\n",
    "                    f\"Evidence (ONLY use for fresh claims; may be empty):\\n\"\n",
    "                    f\"{[e.model_dump() for e in evidence][:16]}\"\n",
    "                )\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return {\"plan\": plan}\n",
    "\n",
    "# -----------------------------\n",
    "# 6) Fanout\n",
    "# -----------------------------\n",
    "def fanout(state: State):\n",
    "    return [\n",
    "        Send(\n",
    "            \"worker\",\n",
    "            {\n",
    "                \"task\": task.model_dump(),\n",
    "                \"topic\": state[\"topic\"],\n",
    "                \"mode\": state[\"mode\"],\n",
    "                \"plan\": state[\"plan\"].model_dump(),\n",
    "                \"evidence\": [e.model_dump() for e in state.get(\"evidence\", [])],\n",
    "            },\n",
    "        )\n",
    "        for task in state[\"plan\"].tasks\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "96b1814f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 7) Worker (write one section)\n",
    "# -----------------------------\n",
    "WORKER_SYSTEM = \"\"\"You are a senior technical writer and developer advocate.\n",
    "Write ONE section of a technical blog post in Markdown.\n",
    "\n",
    "Hard constraints:\n",
    "- Follow the provided Goal and cover ALL Bullets in order (do not skip or merge bullets).\n",
    "- Stay close to Target words (±15%).\n",
    "- Output ONLY the section content in Markdown (no blog title H1, no extra commentary).\n",
    "- Start with a '## <Section Title>' heading.\n",
    "\n",
    "Scope guard:\n",
    "- If blog_kind == \"news_roundup\": do NOT turn this into a tutorial/how-to guide.\n",
    "  Do NOT teach web scraping, RSS, automation, or \"how to fetch news\" unless bullets explicitly ask for it.\n",
    "  Focus on summarizing events and implications.\n",
    "\n",
    "Grounding policy:\n",
    "- If mode == open_book:\n",
    "  - Do NOT introduce any specific event/company/model/funding/policy claim unless it is supported by provided Evidence URLs.\n",
    "  - For each event claim, attach a source as a Markdown link: ([Source](URL)).\n",
    "  - Only use URLs provided in Evidence. If not supported, write: \"Not found in provided sources.\"\n",
    "- If requires_citations == true:\n",
    "  - For outside-world claims, cite Evidence URLs the same way.\n",
    "- Evergreen reasoning is OK without citations unless requires_citations is true.\n",
    "\n",
    "Code:\n",
    "- If requires_code == true, include at least one minimal, correct code snippet relevant to the bullets.\n",
    "\n",
    "Style:\n",
    "- Short paragraphs, bullets where helpful, code fences for code.\n",
    "- Avoid fluff/marketing. Be precise and implementation-oriented.\n",
    "\"\"\"\n",
    "\n",
    "def worker_node(payload: dict) -> dict:\n",
    "    \n",
    "    task = Task(**payload[\"task\"])\n",
    "    plan = Plan(**payload[\"plan\"])\n",
    "    evidence = [EvidenceItem(**e) for e in payload.get(\"evidence\", [])]\n",
    "    topic = payload[\"topic\"]\n",
    "    mode = payload.get(\"mode\", \"closed_book\")\n",
    "\n",
    "    bullets_text = \"\\n- \" + \"\\n- \".join(task.bullets)\n",
    "\n",
    "    evidence_text = \"\"\n",
    "    if evidence:\n",
    "        evidence_text = \"\\n\".join(\n",
    "            f\"- {e.title} | {e.url} | {e.published_at or 'date:unknown'}\".strip()\n",
    "            for e in evidence[:20]\n",
    "        )\n",
    "\n",
    "    section_md = llm.invoke(\n",
    "        [\n",
    "            SystemMessage(content=WORKER_SYSTEM),\n",
    "            HumanMessage(\n",
    "                content=(\n",
    "                    f\"Blog title: {plan.blog_title}\\n\"\n",
    "                    f\"Audience: {plan.audience}\\n\"\n",
    "                    f\"Tone: {plan.tone}\\n\"\n",
    "                    f\"Blog kind: {plan.blog_kind}\\n\"\n",
    "                    f\"Constraints: {plan.constraints}\\n\"\n",
    "                    f\"Topic: {topic}\\n\"\n",
    "                    f\"Mode: {mode}\\n\\n\"\n",
    "                    f\"Section title: {task.title}\\n\"\n",
    "                    f\"Goal: {task.goal}\\n\"\n",
    "                    f\"Target words: {task.target_words}\\n\"\n",
    "                    f\"Tags: {task.tags}\\n\"\n",
    "                    f\"requires_research: {task.requires_research}\\n\"\n",
    "                    f\"requires_citations: {task.requires_citations}\\n\"\n",
    "                    f\"requires_code: {task.requires_code}\\n\"\n",
    "                    f\"Bullets:{bullets_text}\\n\\n\"\n",
    "                    f\"Evidence (ONLY use these URLs when citing):\\n{evidence_text}\\n\"\n",
    "                )\n",
    "            ),\n",
    "        ]\n",
    "    ).content.strip()\n",
    "\n",
    "    return {\"sections\": [(task.id, section_md)]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6a163e5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting deepseek\n",
      "  Downloading deepseek-1.0.0-py3-none-any.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: requests in c:\\users\\jeste\\onedrive\\desktop\\jes\\langgraph\\myenv\\lib\\site-packages (from deepseek) (2.32.5)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\jeste\\onedrive\\desktop\\jes\\langgraph\\myenv\\lib\\site-packages (from requests->deepseek) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\jeste\\onedrive\\desktop\\jes\\langgraph\\myenv\\lib\\site-packages (from requests->deepseek) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\jeste\\onedrive\\desktop\\jes\\langgraph\\myenv\\lib\\site-packages (from requests->deepseek) (2.6.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\jeste\\onedrive\\desktop\\jes\\langgraph\\myenv\\lib\\site-packages (from requests->deepseek) (2026.1.4)\n",
      "Downloading deepseek-1.0.0-py3-none-any.whl (4.5 kB)\n",
      "Installing collected packages: deepseek\n",
      "Successfully installed deepseek-1.0.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.2 -> 26.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install deepseek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a0a60f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 8) ReducerWithImages (subgraph)\n",
    "#    merge_content -> decide_images -> generate_and_place_images\n",
    "# ============================================================\n",
    "def merge_content(state: State) -> dict:\n",
    "\n",
    "    plan = state[\"plan\"]\n",
    "\n",
    "    ordered_sections = [md for _, md in sorted(state[\"sections\"], key=lambda x: x[0])]\n",
    "    body = \"\\n\\n\".join(ordered_sections).strip()\n",
    "    merged_md = f\"# {plan.blog_title}\\n\\n{body}\\n\"\n",
    "    return {\"merged_md\": merged_md}\n",
    "\n",
    "\n",
    "DECIDE_IMAGES_SYSTEM = \"\"\"You are an expert technical editor.\n",
    "Decide if images/diagrams are needed for THIS blog.\n",
    "\n",
    "Rules:\n",
    "- Max 2 images total.\n",
    "- Each image must materially improve understanding (diagram/flow/table-like visual).\n",
    "- Insert placeholders exactly: [[IMAGE_1]], [[IMAGE_2]].\n",
    "- If no images needed: md_with_placeholders must equal input and images=[].\n",
    "- Avoid decorative images; prefer technical diagrams with short labels.\n",
    "Return strictly GlobalImagePlan.\n",
    "\"\"\"\n",
    "\n",
    "def decide_images(state: State) -> dict:\n",
    "    \n",
    "    planner = llm.with_structured_output(GlobalImagePlan)\n",
    "    merged_md = state[\"merged_md\"]\n",
    "    plan = state[\"plan\"]\n",
    "    assert plan is not None\n",
    "\n",
    "    image_plan = planner.invoke(\n",
    "        [\n",
    "            SystemMessage(content=DECIDE_IMAGES_SYSTEM),\n",
    "            HumanMessage(\n",
    "                content=(\n",
    "                    f\"Blog kind: {plan.blog_kind}\\n\"\n",
    "                    f\"Topic: {state['topic']}\\n\\n\"\n",
    "                    \"Insert placeholders + propose image prompts.\\n\\n\"\n",
    "                    f\"{merged_md}\"\n",
    "                )\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"md_with_placeholders\": image_plan.md_with_placeholders,\n",
    "        \"image_specs\": [img.model_dump() for img in image_plan.images],\n",
    "    }\n",
    "\n",
    "\n",
    "def _deepseek_generate_image_bytes(prompt: str) -> bytes:\n",
    "    \"\"\"\n",
    "    Generate image bytes using DeepSeek free image model.\n",
    "    Requires: pip install deepseek\n",
    "    Env var: DEEPSEEK_API_KEY\n",
    "    \"\"\"\n",
    "    import deepseek\n",
    "\n",
    "    api_key = os.environ.get(\"DEEPSEEK_API_KEY\")\n",
    "    if not api_key:\n",
    "        raise RuntimeError(\"DEEPSEEK_API_KEY is not set.\")\n",
    "\n",
    "    client = deepseek.Client(api_key=api_key)\n",
    "\n",
    "    resp = client.generate(\n",
    "        model=\"ds-image-1.0\",  # free image model\n",
    "        prompt=prompt,\n",
    "        size=\"1024x1024\"       \n",
    "    )\n",
    "\n",
    "    if \"image_base64\" not in resp or not resp[\"image_base64\"]:\n",
    "        raise RuntimeError(\"No image returned by DeepSeek.\")\n",
    "\n",
    "    import base64\n",
    "    return base64.b64decode(resp[\"image_base64\"])\n",
    "\n",
    "\n",
    "def generate_and_place_images(state: State) -> dict:\n",
    "\n",
    "    plan = state[\"plan\"]\n",
    "    assert plan is not None\n",
    "\n",
    "    md = state.get(\"md_with_placeholders\") or state[\"merged_md\"]\n",
    "    image_specs = state.get(\"image_specs\", []) or []\n",
    "\n",
    "    # If no images requested, just write merged markdown\n",
    "    if not image_specs:\n",
    "        filename = f\"{plan.blog_title}.md\"\n",
    "        Path(filename).write_text(md, encoding=\"utf-8\")\n",
    "        return {\"final\": md}\n",
    "\n",
    "    images_dir = Path(\"images\")\n",
    "    images_dir.mkdir(exist_ok=True)\n",
    "\n",
    "    for spec in image_specs:\n",
    "        placeholder = spec[\"placeholder\"]\n",
    "        filename = spec[\"filename\"]\n",
    "        out_path = images_dir / filename\n",
    "\n",
    "        # generate only if needed\n",
    "        if not out_path.exists():\n",
    "            try:\n",
    "                img_bytes = _deepseek_generate_image_bytes(spec[\"prompt\"])\n",
    "                out_path.write_bytes(img_bytes)\n",
    "            except Exception as e:\n",
    "                # graceful fallback: keep doc usable\n",
    "                prompt_block = (\n",
    "                    f\"> **[IMAGE GENERATION FAILED]** {spec.get('caption','')}\\n>\\n\"\n",
    "                    f\"> **Alt:** {spec.get('alt','')}\\n>\\n\"\n",
    "                    f\"> **Prompt:** {spec.get('prompt','')}\\n>\\n\"\n",
    "                    f\"> **Error:** {e}\\n\"\n",
    "                )\n",
    "                md = md.replace(placeholder, prompt_block)\n",
    "                continue\n",
    "\n",
    "        img_md = f\"![{spec['alt']}](images/{filename})\\n*{spec['caption']}*\"\n",
    "        md = md.replace(placeholder, img_md)\n",
    "\n",
    "    filename = f\"{plan.blog_title}.md\"\n",
    "    Path(filename).write_text(md, encoding=\"utf-8\")\n",
    "    return {\"final\": md}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aa21c3c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<langgraph.graph.state.CompiledStateGraph object at 0x0000029910220830>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# build reducer subgraph\n",
    "reducer_graph = StateGraph(State)\n",
    "reducer_graph.add_node(\"merge_content\", merge_content)\n",
    "reducer_graph.add_node(\"decide_images\", decide_images)\n",
    "reducer_graph.add_node(\"generate_and_place_images\", generate_and_place_images)\n",
    "reducer_graph.add_edge(START, \"merge_content\")\n",
    "reducer_graph.add_edge(\"merge_content\", \"decide_images\")\n",
    "reducer_graph.add_edge(\"decide_images\", \"generate_and_place_images\")\n",
    "reducer_graph.add_edge(\"generate_and_place_images\", END)\n",
    "reducer_subgraph = reducer_graph.compile()\n",
    "\n",
    "reducer_subgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "32deb553",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<langgraph.graph.state.CompiledStateGraph object at 0x000002991021D1D0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# 9) Build main graph\n",
    "# -----------------------------\n",
    "g = StateGraph(State)\n",
    "g.add_node(\"router\", router_node)\n",
    "g.add_node(\"research\", research_node)\n",
    "g.add_node(\"orchestrator\", orchestrator_node)\n",
    "g.add_node(\"worker\", worker_node)\n",
    "g.add_node(\"reducer\", reducer_subgraph)\n",
    "\n",
    "g.add_edge(START, \"router\")\n",
    "g.add_conditional_edges(\"router\", route_next, {\"research\": \"research\", \"orchestrator\": \"orchestrator\"})\n",
    "g.add_edge(\"research\", \"orchestrator\")\n",
    "\n",
    "g.add_conditional_edges(\"orchestrator\", fanout, [\"worker\"])\n",
    "g.add_edge(\"worker\", \"reducer\")\n",
    "g.add_edge(\"reducer\", END)\n",
    "\n",
    "app = g.compile()\n",
    "app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d01021d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 10) Runner\n",
    "# -----------------------------\n",
    "def run(topic: str, as_of: Optional[str] = None):\n",
    "    if as_of is None:\n",
    "        as_of = date.today().isoformat()\n",
    "\n",
    "    out = app.invoke(\n",
    "        {\n",
    "            \"topic\": topic,\n",
    "            \"mode\": \"\",\n",
    "            \"needs_research\": False,\n",
    "            \"queries\": [],\n",
    "            \"evidence\": [],\n",
    "            \"plan\": None,\n",
    "            \"as_of\": as_of,\n",
    "            \"recency_days\": 7,\n",
    "            \"sections\": [],\n",
    "            \"merged_md\": \"\",\n",
    "            \"md_with_placeholders\": \"\",\n",
    "            \"image_specs\": [],\n",
    "            \"final\": \"\",\n",
    "        }\n",
    "    )\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ddaaab83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'topic': 'Self Attention in Transformer Architecture',\n",
       " 'mode': 'closed_book',\n",
       " 'needs_research': False,\n",
       " 'queries': [],\n",
       " 'evidence': [],\n",
       " 'plan': Plan(blog_title='Understanding Self Attention in Transformer Architecture', audience='developers', tone='technical', blog_kind='explainer', constraints=[], tasks=[Task(id=1, title='Introduction to Self Attention', goal='Understand the concept of self attention and its role in transformer architecture.', bullets=['Define self attention and its purpose in processing sequential data', 'Compare self attention with traditional attention mechanisms', 'Discuss the benefits of self attention in parallelizing sequential computations'], target_words=200, tags=['transformer', 'self attention', 'parallelization'], requires_research=False, requires_citations=False, requires_code=False), Task(id=2, title='Mathematical Formulation of Self Attention', goal='Learn the mathematical formulation of self attention and its components.', bullets=['Derive the self attention equation and explain its components', 'Discuss the role of query, key, and value vectors in self attention', 'Measure the computational complexity of self attention'], target_words=300, tags=['mathematics', 'self attention', 'computational complexity'], requires_research=False, requires_citations=False, requires_code=False), Task(id=3, title='Implementing Self Attention', goal='Implement a basic self attention mechanism using a programming language.', bullets=['Write a minimal code sketch to demonstrate self attention', 'Build a simple self attention layer using a deep learning framework', 'Verify the correctness of the self attention implementation'], target_words=250, tags=['implementation', 'self attention', 'deep learning'], requires_research=False, requires_citations=False, requires_code=True), Task(id=4, title='Edge Cases and Performance Considerations', goal='Understand the edge cases and performance considerations of self attention.', bullets=['Discuss the edge cases of self attention, such as zero-length sequences', 'Analyze the performance of self attention on large sequences', 'Compare the performance of self attention with other attention mechanisms'], target_words=220, tags=['edge cases', 'performance', 'self attention'], requires_research=False, requires_citations=False, requires_code=False), Task(id=5, title='Debugging and Optimizing Self Attention', goal='Learn how to debug and optimize self attention mechanisms.', bullets=['Debug common issues with self attention, such as NaN values', 'Optimize self attention for better performance and efficiency', 'Discuss techniques for visualizing self attention weights'], target_words=280, tags=['debugging', 'optimization', 'self attention'], requires_research=False, requires_citations=False, requires_code=False)]),\n",
       " 'sections': [(1,\n",
       "   '## Introduction to Self Attention\\nSelf attention is a key component in transformer architecture, enabling the model to weigh the importance of different input elements relative to each other. \\n* It is defined as a mechanism that allows the model to attend to all positions in the input sequence simultaneously and weigh their importance, thus enabling the capture of long-range dependencies.\\n* Unlike traditional attention mechanisms, which focus on a single input element at a time, self attention considers the entire input sequence, making it more efficient in processing sequential data.\\n* The benefits of self attention include its ability to parallelize sequential computations, making it highly efficient for large-scale processing tasks, which is a significant advantage over traditional recurrent neural network (RNN) architectures that process sequences sequentially.'),\n",
       "  (2,\n",
       "   \"## Mathematical Formulation of Self Attention\\nThe self attention mechanism is a core component of the Transformer architecture, allowing the model to attend to different parts of the input sequence simultaneously. To understand self attention, it's essential to derive the self attention equation and explain its components. The self attention equation is given by:\\nAttention(Q, K, V) = softmax(Q * K^T / sqrt(d)) * V\\nwhere Q, K, and V are the query, key, and value vectors, respectively, and d is the dimensionality of the input sequence.\\n\\n* The query vector Q represents the context in which the attention is being applied.\\n* The key vector K represents the information being attended to.\\n* The value vector V represents the importance of the information being attended to.\\nThe role of these vectors is crucial in self attention, as they allow the model to compute the attention weights, which are used to compute the weighted sum of the value vectors.\\n\\nThe computational complexity of self attention is O(n^2 * d), where n is the length of the input sequence and d is the dimensionality of the input sequence. This is because the self attention mechanism involves computing the dot product of the query and key vectors, which has a complexity of O(n^2), and then computing the softmax function, which has a complexity of O(n). The value vector is then computed by taking the weighted sum of the value vectors, which has a complexity of O(n * d). Overall, the self attention mechanism provides a powerful way to model complex relationships between different parts of the input sequence, but its computational complexity can be a limitation for long input sequences.\"),\n",
       "  (3,\n",
       "   '## Implementing Self Attention\\nTo implement a basic self attention mechanism, we can start by writing a minimal code sketch. \\nThis involves calculating the query, key, and value matrices from the input, \\nthen computing the attention weights and applying them to the value matrix.\\n\\n* A minimal code sketch for self attention can be demonstrated using the following Python code:\\n```python\\nimport torch\\nimport torch.nn as nn\\nimport torch.nn.functional as F\\n\\nclass SelfAttention(nn.Module):\\n    def __init__(self, embed_dim):\\n        super(SelfAttention, self).__init__()\\n        self.query_linear = nn.Linear(embed_dim, embed_dim)\\n        self.key_linear = nn.Linear(embed_dim, embed_dim)\\n        self.value_linear = nn.Linear(embed_dim, embed_dim)\\n\\n    def forward(self, x):\\n        query = self.query_linear(x)\\n        key = self.key_linear(x)\\n        value = self.value_linear(x)\\n        attention_weights = F.softmax(torch.matmul(query, key.T) / math.sqrt(key.shape[1]), dim=-1)\\n        output = torch.matmul(attention_weights, value)\\n        return output\\n```\\nWe can build a simple self attention layer using a deep learning framework like PyTorch. \\nThe `SelfAttention` class defines a self attention mechanism with learnable linear transformations for the query, key, and value matrices.\\n\\nTo verify the correctness of the self attention implementation, we can test it with a random input tensor and check the output shape and values. \\nThis ensures that the self attention layer is functioning as expected and can be used as a building block for more complex transformer architectures.'),\n",
       "  (4,\n",
       "   '## Edge Cases and Performance Considerations\\nSelf attention in Transformer architecture has several edge cases that need to be considered. \\n* Zero-length sequences are one such edge case, where the model needs to handle empty input sequences.\\nWhen dealing with zero-length sequences, the self attention mechanism may not be able to capture any meaningful information, as there are no elements to attend to.\\nIn terms of performance, self attention can be computationally expensive, especially for large sequences. \\n* Analyzing the performance of self attention on large sequences reveals that it can lead to increased computational costs and memory usage.\\n* Comparing the performance of self attention with other attention mechanisms, such as local attention or hierarchical attention, can provide insights into its strengths and weaknesses.\\nOverall, understanding these edge cases and performance considerations is crucial for effective implementation and optimization of self attention in Transformer architecture.'),\n",
       "  (5,\n",
       "   \"## Debugging and Optimizing Self Attention\\nDebugging self attention mechanisms can be challenging, but there are common issues to look out for, such as NaN values. These can occur due to exploding gradients or vanishing weights, and can be addressed by implementing gradient clipping or weight normalization.\\n\\nTo optimize self attention for better performance and efficiency, several techniques can be employed. These include using attention dropout to prevent overfitting, and applying layer normalization to stabilize the training process.\\n\\nTechniques for visualizing self attention weights can provide valuable insights into the model's behavior. This can be achieved through the use of heatmaps or attention maps, which illustrate the relative importance of different input elements. By analyzing these visualizations, developers can gain a deeper understanding of how the self attention mechanism is operating, and make targeted adjustments to improve its performance.\"),\n",
       "  (1,\n",
       "   '## Introduction to Self Attention\\nSelf attention is a key component in transformer architecture, enabling the model to weigh the importance of different input elements relative to each other. \\n* It is defined as a mechanism that allows the model to attend to all positions in the input sequence simultaneously and weigh their importance, thus enabling the capture of long-range dependencies.\\n* Unlike traditional attention mechanisms, which focus on a single input element at a time, self attention considers the entire input sequence, making it more efficient in processing sequential data.\\n* The benefits of self attention include its ability to parallelize sequential computations, making it highly efficient for large-scale processing tasks, which is a significant advantage over traditional recurrent neural network (RNN) architectures that process sequences sequentially.'),\n",
       "  (2,\n",
       "   \"## Mathematical Formulation of Self Attention\\nThe self attention mechanism is a core component of the Transformer architecture, allowing the model to attend to different parts of the input sequence simultaneously. To understand self attention, it's essential to derive the self attention equation and explain its components. The self attention equation is given by:\\nAttention(Q, K, V) = softmax(Q * K^T / sqrt(d)) * V\\nwhere Q, K, and V are the query, key, and value vectors, respectively, and d is the dimensionality of the input sequence.\\n\\n* The query vector Q represents the context in which the attention is being applied.\\n* The key vector K represents the information being attended to.\\n* The value vector V represents the importance of the information being attended to.\\nThe role of these vectors is crucial in self attention, as they allow the model to compute the attention weights, which are used to compute the weighted sum of the value vectors.\\n\\nThe computational complexity of self attention is O(n^2 * d), where n is the length of the input sequence and d is the dimensionality of the input sequence. This is because the self attention mechanism involves computing the dot product of the query and key vectors, which has a complexity of O(n^2), and then computing the softmax function, which has a complexity of O(n). The value vector is then computed by taking the weighted sum of the value vectors, which has a complexity of O(n * d). Overall, the self attention mechanism provides a powerful way to model complex relationships between different parts of the input sequence, but its computational complexity can be a limitation for long input sequences.\"),\n",
       "  (3,\n",
       "   '## Implementing Self Attention\\nTo implement a basic self attention mechanism, we can start by writing a minimal code sketch. \\nThis involves calculating the query, key, and value matrices from the input, \\nthen computing the attention weights and applying them to the value matrix.\\n\\n* A minimal code sketch for self attention can be demonstrated using the following Python code:\\n```python\\nimport torch\\nimport torch.nn as nn\\nimport torch.nn.functional as F\\n\\nclass SelfAttention(nn.Module):\\n    def __init__(self, embed_dim):\\n        super(SelfAttention, self).__init__()\\n        self.query_linear = nn.Linear(embed_dim, embed_dim)\\n        self.key_linear = nn.Linear(embed_dim, embed_dim)\\n        self.value_linear = nn.Linear(embed_dim, embed_dim)\\n\\n    def forward(self, x):\\n        query = self.query_linear(x)\\n        key = self.key_linear(x)\\n        value = self.value_linear(x)\\n        attention_weights = F.softmax(torch.matmul(query, key.T) / math.sqrt(key.shape[1]), dim=-1)\\n        output = torch.matmul(attention_weights, value)\\n        return output\\n```\\nWe can build a simple self attention layer using a deep learning framework like PyTorch. \\nThe `SelfAttention` class defines a self attention mechanism with learnable linear transformations for the query, key, and value matrices.\\n\\nTo verify the correctness of the self attention implementation, we can test it with a random input tensor and check the output shape and values. \\nThis ensures that the self attention layer is functioning as expected and can be used as a building block for more complex transformer architectures.'),\n",
       "  (4,\n",
       "   '## Edge Cases and Performance Considerations\\nSelf attention in Transformer architecture has several edge cases that need to be considered. \\n* Zero-length sequences are one such edge case, where the model needs to handle empty input sequences.\\nWhen dealing with zero-length sequences, the self attention mechanism may not be able to capture any meaningful information, as there are no elements to attend to.\\nIn terms of performance, self attention can be computationally expensive, especially for large sequences. \\n* Analyzing the performance of self attention on large sequences reveals that it can lead to increased computational costs and memory usage.\\n* Comparing the performance of self attention with other attention mechanisms, such as local attention or hierarchical attention, can provide insights into its strengths and weaknesses.\\nOverall, understanding these edge cases and performance considerations is crucial for effective implementation and optimization of self attention in Transformer architecture.'),\n",
       "  (5,\n",
       "   \"## Debugging and Optimizing Self Attention\\nDebugging self attention mechanisms can be challenging, but there are common issues to look out for, such as NaN values. These can occur due to exploding gradients or vanishing weights, and can be addressed by implementing gradient clipping or weight normalization.\\n\\nTo optimize self attention for better performance and efficiency, several techniques can be employed. These include using attention dropout to prevent overfitting, and applying layer normalization to stabilize the training process.\\n\\nTechniques for visualizing self attention weights can provide valuable insights into the model's behavior. This can be achieved through the use of heatmaps or attention maps, which illustrate the relative importance of different input elements. By analyzing these visualizations, developers can gain a deeper understanding of how the self attention mechanism is operating, and make targeted adjustments to improve its performance.\")],\n",
       " 'merged_md': \"# Understanding Self Attention in Transformer Architecture\\n\\n## Introduction to Self Attention\\nSelf attention is a key component in transformer architecture, enabling the model to weigh the importance of different input elements relative to each other. \\n* It is defined as a mechanism that allows the model to attend to all positions in the input sequence simultaneously and weigh their importance, thus enabling the capture of long-range dependencies.\\n* Unlike traditional attention mechanisms, which focus on a single input element at a time, self attention considers the entire input sequence, making it more efficient in processing sequential data.\\n* The benefits of self attention include its ability to parallelize sequential computations, making it highly efficient for large-scale processing tasks, which is a significant advantage over traditional recurrent neural network (RNN) architectures that process sequences sequentially.\\n\\n## Mathematical Formulation of Self Attention\\nThe self attention mechanism is a core component of the Transformer architecture, allowing the model to attend to different parts of the input sequence simultaneously. To understand self attention, it's essential to derive the self attention equation and explain its components. The self attention equation is given by:\\nAttention(Q, K, V) = softmax(Q * K^T / sqrt(d)) * V\\nwhere Q, K, and V are the query, key, and value vectors, respectively, and d is the dimensionality of the input sequence.\\n\\n* The query vector Q represents the context in which the attention is being applied.\\n* The key vector K represents the information being attended to.\\n* The value vector V represents the importance of the information being attended to.\\nThe role of these vectors is crucial in self attention, as they allow the model to compute the attention weights, which are used to compute the weighted sum of the value vectors.\\n\\nThe computational complexity of self attention is O(n^2 * d), where n is the length of the input sequence and d is the dimensionality of the input sequence. This is because the self attention mechanism involves computing the dot product of the query and key vectors, which has a complexity of O(n^2), and then computing the softmax function, which has a complexity of O(n). The value vector is then computed by taking the weighted sum of the value vectors, which has a complexity of O(n * d). Overall, the self attention mechanism provides a powerful way to model complex relationships between different parts of the input sequence, but its computational complexity can be a limitation for long input sequences.\\n\\n## Implementing Self Attention\\nTo implement a basic self attention mechanism, we can start by writing a minimal code sketch. \\nThis involves calculating the query, key, and value matrices from the input, \\nthen computing the attention weights and applying them to the value matrix.\\n\\n* A minimal code sketch for self attention can be demonstrated using the following Python code:\\n```python\\nimport torch\\nimport torch.nn as nn\\nimport torch.nn.functional as F\\n\\nclass SelfAttention(nn.Module):\\n    def __init__(self, embed_dim):\\n        super(SelfAttention, self).__init__()\\n        self.query_linear = nn.Linear(embed_dim, embed_dim)\\n        self.key_linear = nn.Linear(embed_dim, embed_dim)\\n        self.value_linear = nn.Linear(embed_dim, embed_dim)\\n\\n    def forward(self, x):\\n        query = self.query_linear(x)\\n        key = self.key_linear(x)\\n        value = self.value_linear(x)\\n        attention_weights = F.softmax(torch.matmul(query, key.T) / math.sqrt(key.shape[1]), dim=-1)\\n        output = torch.matmul(attention_weights, value)\\n        return output\\n```\\nWe can build a simple self attention layer using a deep learning framework like PyTorch. \\nThe `SelfAttention` class defines a self attention mechanism with learnable linear transformations for the query, key, and value matrices.\\n\\nTo verify the correctness of the self attention implementation, we can test it with a random input tensor and check the output shape and values. \\nThis ensures that the self attention layer is functioning as expected and can be used as a building block for more complex transformer architectures.\\n\\n## Edge Cases and Performance Considerations\\nSelf attention in Transformer architecture has several edge cases that need to be considered. \\n* Zero-length sequences are one such edge case, where the model needs to handle empty input sequences.\\nWhen dealing with zero-length sequences, the self attention mechanism may not be able to capture any meaningful information, as there are no elements to attend to.\\nIn terms of performance, self attention can be computationally expensive, especially for large sequences. \\n* Analyzing the performance of self attention on large sequences reveals that it can lead to increased computational costs and memory usage.\\n* Comparing the performance of self attention with other attention mechanisms, such as local attention or hierarchical attention, can provide insights into its strengths and weaknesses.\\nOverall, understanding these edge cases and performance considerations is crucial for effective implementation and optimization of self attention in Transformer architecture.\\n\\n## Debugging and Optimizing Self Attention\\nDebugging self attention mechanisms can be challenging, but there are common issues to look out for, such as NaN values. These can occur due to exploding gradients or vanishing weights, and can be addressed by implementing gradient clipping or weight normalization.\\n\\nTo optimize self attention for better performance and efficiency, several techniques can be employed. These include using attention dropout to prevent overfitting, and applying layer normalization to stabilize the training process.\\n\\nTechniques for visualizing self attention weights can provide valuable insights into the model's behavior. This can be achieved through the use of heatmaps or attention maps, which illustrate the relative importance of different input elements. By analyzing these visualizations, developers can gain a deeper understanding of how the self attention mechanism is operating, and make targeted adjustments to improve its performance.\\n\",\n",
       " 'md_with_placeholders': \"# Understanding Self Attention in Transformer Architecture\\n## Introduction to Self Attention\\nSelf attention is a key component in transformer architecture, enabling the model to weigh the importance of different input elements relative to each other. \\n* It is defined as a mechanism that allows the model to attend to all positions in the input sequence simultaneously and weigh their importance, thus enabling the capture of long-range dependencies.\\n* Unlike traditional attention mechanisms, which focus on a single input element at a time, self attention considers the entire input sequence, making it more efficient in processing sequential data.\\n* The benefits of self attention include its ability to parallelize sequential computations, making it highly efficient for large-scale processing tasks, which is a significant advantage over traditional recurrent neural network (RNN) architectures that process sequences sequentially.\\n## Mathematical Formulation of Self Attention\\nThe self attention mechanism is a core component of the Transformer architecture, allowing the model to attend to different parts of the input sequence simultaneously. To understand self attention, it's essential to derive the self attention equation and explain its components. The self attention equation is given by:\\nAttention(Q, K, V) = softmax(Q * K^T / sqrt(d)) * V\\nwhere Q, K, and V are the query, key, and value vectors, respectively, and d is the dimensionality of the input sequence.\\n* The query vector Q represents the context in which the attention is being applied.\\n* The key vector K represents the information being attended to.\\n* The value vector V represents the importance of the information being attended to.\\nThe role of these vectors is crucial in self attention, as they allow the model to compute the attention weights, which are used to compute the weighted sum of the value vectors.\\n[[IMAGE_1]]\\nThe computational complexity of self attention is O(n^2 * d), where n is the length of the input sequence and d is the dimensionality of the input sequence. This is because the self attention mechanism involves computing the dot product of the query and key vectors, which has a complexity of O(n^2), and then computing the softmax function, which has a complexity of O(n). The value vector is then computed by taking the weighted sum of the value vectors, which has a complexity of O(n * d). Overall, the self attention mechanism provides a powerful way to model complex relationships between different parts of the input sequence, but its computational complexity can be a limitation for long input sequences.\\n## Implementing Self Attention\\nTo implement a basic self attention mechanism, we can start by writing a minimal code sketch. \\nThis involves calculating the query, key, and value matrices from the input, \\nthen computing the attention weights and applying them to the value matrix.\\n* A minimal code sketch for self attention can be demonstrated using the following Python code:\\n```python\\nimport torch\\nimport torch.nn as nn\\nimport torch.nn.functional as F\\n\\nclass SelfAttention(nn.Module):\\n    def __init__(self, embed_dim):\\n        super(SelfAttention, self).__init__()\\n        self.query_linear = nn.Linear(embed_dim, embed_dim)\\n        self.key_linear = nn.Linear(embed_dim, embed_dim)\\n        self.value_linear = nn.Linear(embed_dim, embed_dim)\\n\\n    def forward(self, x):\\n        query = self.query_linear(x)\\n        key = self.key_linear(x)\\n        value = self.value_linear(x)\\n        attention_weights = F.softmax(torch.matmul(query, key.T) / math.sqrt(key.shape[1]), dim=-1)\\n        output = torch.matmul(attention_weights, value)\\n        return output\\n```\\nWe can build a simple self attention layer using a deep learning framework like PyTorch. \\nThe `SelfAttention` class defines a self attention mechanism with learnable linear transformations for the query, key, and value matrices.\\n[[IMAGE_2]]\\nTo verify the correctness of the self attention implementation, we can test it with a random input tensor and check the output shape and values. \\nThis ensures that the self attention layer is functioning as expected and can be used as a building block for more complex transformer architectures.\\n## Edge Cases and Performance Considerations\\nSelf attention in Transformer architecture has several edge cases that need to be considered. \\n* Zero-length sequences are one such edge case, where the model needs to handle empty input sequences.\\nWhen dealing with zero-length sequences, the self attention mechanism may not be able to capture any meaningful information, as there are no elements to attend to.\\nIn terms of performance, self attention can be computationally expensive, especially for large sequences. \\n* Analyzing the performance of self attention on large sequences reveals that it can lead to increased computational costs and memory usage.\\n* Comparing the performance of self attention with other attention mechanisms, such as local attention or hierarchical attention, can provide insights into its strengths and weaknesses.\\nOverall, understanding these edge cases and performance considerations is crucial for effective implementation and optimization of self attention in Transformer architecture.\\n## Debugging and Optimizing Self Attention\\nDebugging self attention mechanisms can be challenging, but there are common issues to look out for, such as NaN values. These can occur due to exploding gradients or vanishing weights, and can be addressed by implementing gradient clipping or weight normalization.\\nTo optimize self attention for better performance and efficiency, several techniques can be employed. These include using attention dropout to prevent overfitting, and applying layer normalization to stabilize the training process.\\nTechniques for visualizing self attention weights can provide valuable insights into the model's behavior. This can be achieved through the use of heatmaps or attention maps, which illustrate the relative importance of different input elements. By analyzing these visualizations, developers can gain a deeper understanding of how the self attention mechanism is operating, and make targeted adjustments to improve its performance.\",\n",
       " 'image_specs': [{'placeholder': '[[IMAGE_1]]',\n",
       "   'filename': 'self_attention_mechanism.png',\n",
       "   'alt': 'Self Attention Mechanism',\n",
       "   'caption': 'Self attention mechanism in Transformer architecture',\n",
       "   'prompt': 'A diagram illustrating the self attention mechanism in Transformer architecture, including the query, key, and value vectors, and the attention weights.',\n",
       "   'size': '1024x1024',\n",
       "   'quality': 'high'},\n",
       "  {'placeholder': '[[IMAGE_2]]',\n",
       "   'filename': 'self_attention_implementation.png',\n",
       "   'alt': 'Self Attention Implementation',\n",
       "   'caption': 'Self attention implementation using PyTorch',\n",
       "   'prompt': 'A code snippet or diagram illustrating the implementation of self attention using PyTorch, including the calculation of query, key, and value matrices, and the computation of attention weights.',\n",
       "   'size': '1024x1024',\n",
       "   'quality': 'high'}],\n",
       " 'final': \"# Understanding Self Attention in Transformer Architecture\\n## Introduction to Self Attention\\nSelf attention is a key component in transformer architecture, enabling the model to weigh the importance of different input elements relative to each other. \\n* It is defined as a mechanism that allows the model to attend to all positions in the input sequence simultaneously and weigh their importance, thus enabling the capture of long-range dependencies.\\n* Unlike traditional attention mechanisms, which focus on a single input element at a time, self attention considers the entire input sequence, making it more efficient in processing sequential data.\\n* The benefits of self attention include its ability to parallelize sequential computations, making it highly efficient for large-scale processing tasks, which is a significant advantage over traditional recurrent neural network (RNN) architectures that process sequences sequentially.\\n## Mathematical Formulation of Self Attention\\nThe self attention mechanism is a core component of the Transformer architecture, allowing the model to attend to different parts of the input sequence simultaneously. To understand self attention, it's essential to derive the self attention equation and explain its components. The self attention equation is given by:\\nAttention(Q, K, V) = softmax(Q * K^T / sqrt(d)) * V\\nwhere Q, K, and V are the query, key, and value vectors, respectively, and d is the dimensionality of the input sequence.\\n* The query vector Q represents the context in which the attention is being applied.\\n* The key vector K represents the information being attended to.\\n* The value vector V represents the importance of the information being attended to.\\nThe role of these vectors is crucial in self attention, as they allow the model to compute the attention weights, which are used to compute the weighted sum of the value vectors.\\n> **[IMAGE GENERATION FAILED]** Self attention mechanism in Transformer architecture\\n>\\n> **Alt:** Self Attention Mechanism\\n>\\n> **Prompt:** A diagram illustrating the self attention mechanism in Transformer architecture, including the query, key, and value vectors, and the attention weights.\\n>\\n> **Error:** module 'deepseek' has no attribute 'Client'\\n\\nThe computational complexity of self attention is O(n^2 * d), where n is the length of the input sequence and d is the dimensionality of the input sequence. This is because the self attention mechanism involves computing the dot product of the query and key vectors, which has a complexity of O(n^2), and then computing the softmax function, which has a complexity of O(n). The value vector is then computed by taking the weighted sum of the value vectors, which has a complexity of O(n * d). Overall, the self attention mechanism provides a powerful way to model complex relationships between different parts of the input sequence, but its computational complexity can be a limitation for long input sequences.\\n## Implementing Self Attention\\nTo implement a basic self attention mechanism, we can start by writing a minimal code sketch. \\nThis involves calculating the query, key, and value matrices from the input, \\nthen computing the attention weights and applying them to the value matrix.\\n* A minimal code sketch for self attention can be demonstrated using the following Python code:\\n```python\\nimport torch\\nimport torch.nn as nn\\nimport torch.nn.functional as F\\n\\nclass SelfAttention(nn.Module):\\n    def __init__(self, embed_dim):\\n        super(SelfAttention, self).__init__()\\n        self.query_linear = nn.Linear(embed_dim, embed_dim)\\n        self.key_linear = nn.Linear(embed_dim, embed_dim)\\n        self.value_linear = nn.Linear(embed_dim, embed_dim)\\n\\n    def forward(self, x):\\n        query = self.query_linear(x)\\n        key = self.key_linear(x)\\n        value = self.value_linear(x)\\n        attention_weights = F.softmax(torch.matmul(query, key.T) / math.sqrt(key.shape[1]), dim=-1)\\n        output = torch.matmul(attention_weights, value)\\n        return output\\n```\\nWe can build a simple self attention layer using a deep learning framework like PyTorch. \\nThe `SelfAttention` class defines a self attention mechanism with learnable linear transformations for the query, key, and value matrices.\\n> **[IMAGE GENERATION FAILED]** Self attention implementation using PyTorch\\n>\\n> **Alt:** Self Attention Implementation\\n>\\n> **Prompt:** A code snippet or diagram illustrating the implementation of self attention using PyTorch, including the calculation of query, key, and value matrices, and the computation of attention weights.\\n>\\n> **Error:** module 'deepseek' has no attribute 'Client'\\n\\nTo verify the correctness of the self attention implementation, we can test it with a random input tensor and check the output shape and values. \\nThis ensures that the self attention layer is functioning as expected and can be used as a building block for more complex transformer architectures.\\n## Edge Cases and Performance Considerations\\nSelf attention in Transformer architecture has several edge cases that need to be considered. \\n* Zero-length sequences are one such edge case, where the model needs to handle empty input sequences.\\nWhen dealing with zero-length sequences, the self attention mechanism may not be able to capture any meaningful information, as there are no elements to attend to.\\nIn terms of performance, self attention can be computationally expensive, especially for large sequences. \\n* Analyzing the performance of self attention on large sequences reveals that it can lead to increased computational costs and memory usage.\\n* Comparing the performance of self attention with other attention mechanisms, such as local attention or hierarchical attention, can provide insights into its strengths and weaknesses.\\nOverall, understanding these edge cases and performance considerations is crucial for effective implementation and optimization of self attention in Transformer architecture.\\n## Debugging and Optimizing Self Attention\\nDebugging self attention mechanisms can be challenging, but there are common issues to look out for, such as NaN values. These can occur due to exploding gradients or vanishing weights, and can be addressed by implementing gradient clipping or weight normalization.\\nTo optimize self attention for better performance and efficiency, several techniques can be employed. These include using attention dropout to prevent overfitting, and applying layer normalization to stabilize the training process.\\nTechniques for visualizing self attention weights can provide valuable insights into the model's behavior. This can be achieved through the use of heatmaps or attention maps, which illustrate the relative importance of different input elements. By analyzing these visualizations, developers can gain a deeper understanding of how the self attention mechanism is operating, and make targeted adjustments to improve its performance.\"}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run(\"Self Attention in Transformer Architecture\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aeed2af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
